{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "x = nd.random_uniform(shape=(2000, 2000))\n",
    "y = nd.dot(x, x)\n",
    "print('=== workloads are pushed into the backend engine ===\\n%f sec' % (time() - start))\n",
    "z = y.asnumpy()\n",
    "print('=== workloads are finished ===\\n%f sec' % (time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(x):\n",
    "    return [nd.dot(x, x) for i in range(10)]\n",
    "\n",
    "def wait(x):\n",
    "    for y in x:\n",
    "        y.wait_to_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = nd.random_uniform(shape=(4000, 4000), ctx=gpu(0))\n",
    "x1 = x0.copyto(gpu(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Run on GPU 0 and 1 in sequential ===')\n",
    "start = time()\n",
    "wait(run(x0))\n",
    "wait(run(x1))\n",
    "print('time: %f sec' %(time() - start))\n",
    "\n",
    "print('=== Run on GPU 0 and 1 in parallel ===')\n",
    "start = time()\n",
    "y0 = run(x0)\n",
    "y1 = run(x1)\n",
    "wait(y0)\n",
    "wait(y1)\n",
    "print('time: %f sec' %(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import cpu\n",
    "\n",
    "def copy(x, ctx):\n",
    "    \"\"\"copy data to a device\"\"\"\n",
    "    return [y.copyto(ctx) for y in x]\n",
    "\n",
    "print('=== Run on GPU 0 and then copy results to CPU in sequential ===')\n",
    "start = time()\n",
    "y0 = run(x0)\n",
    "wait(y0)\n",
    "z0 = copy(y0, cpu())\n",
    "wait(z0)\n",
    "print(time() - start)\n",
    "\n",
    "print('=== Run and copy in parallel ===')\n",
    "start = time()\n",
    "y0 = run(x0)\n",
    "z0 = copy(y0, cpu())\n",
    "wait(z0)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "# initialize parameters\n",
    "scale = .01\n",
    "W1 = nd.random_normal(shape=(20,1,3,3))*scale\n",
    "b1 = nd.zeros(shape=20)\n",
    "W2 = nd.random_normal(shape=(50,20,5,5))*scale\n",
    "b2 = nd.zeros(shape=50)\n",
    "W3 = nd.random_normal(shape=(800,128))*scale\n",
    "b3 = nd.zeros(shape=128)\n",
    "W4 = nd.random_normal(shape=(128,10))*scale\n",
    "b4 = nd.zeros(shape=10)\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "# network and loss\n",
    "def lenet(X, params):\n",
    "    # first conv\n",
    "    h1_conv = nd.Convolution(data=X, weight=params[0], bias=params[1], kernel=(3,3), num_filter=20)\n",
    "    h1_activation = nd.relu(h1_conv)\n",
    "    h1 = nd.Pooling(data=h1_activation, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "    # second conv\n",
    "    h2_conv = nd.Convolution(data=h1, weight=params[2], bias=params[3], kernel=(5,5), num_filter=50)\n",
    "    h2_activation = nd.relu(h2_conv)\n",
    "    h2 = nd.Pooling(data=h2_activation, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "    h2 = nd.flatten(h2)\n",
    "    # first fullc\n",
    "    h3_linear = nd.dot(h2, params[4]) + params[5]\n",
    "    h3 = nd.relu(h3_linear)\n",
    "    # second fullc\n",
    "    yhat = nd.dot(h3, params[6]) + params[7]\n",
    "    return yhat\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# plain SGD\n",
    "def SGD(params, lr):\n",
    "    for p in params:\n",
    "        p[:] = p - lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(params, ctx):\n",
    "    new_params = [p.copyto(ctx) for p in params]\n",
    "    for p in new_params:\n",
    "        p.attach_grad()\n",
    "    return new_params\n",
    "\n",
    "new_params = get_params(params, gpu(0))\n",
    "print('=== copy b1 to GPU(0) ===\\nweight = {}\\ngrad = {}'.format(\n",
    "    new_params[1], new_params[1].grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allreduce(data):\n",
    "    # sum on data[0].context, and then broadcast\n",
    "    for i in range(1, len(data)):\n",
    "        data[0][:] += data[i].copyto(data[0].context)\n",
    "    for i in range(1, len(data)):\n",
    "        data[0].copyto(data[i])\n",
    "\n",
    "data = [nd.ones((1,2), ctx=gpu(i))*(i+1) for i in range(2)]\n",
    "print(\"=== before allreduce ===\\n {}\".format(data))\n",
    "allreduce(data)\n",
    "print(\"\\n=== after allreduce ===\\n {}\".format(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_load(data, ctx):\n",
    "    n, k = data.shape[0], len(ctx)\n",
    "    assert (n//k)*k == n, '# examples is not divided by # devices'\n",
    "    idx = list(range(0, n+1, n//k))\n",
    "    return [data[idx[i]:idx[i+1]].as_in_context(ctx[i]) for i in range(k)]\n",
    "\n",
    "batch = nd.arange(16).reshape((4,4))\n",
    "print('=== original data ==={}'.format(batch))\n",
    "ctx = [gpu(0), gpu(1)]\n",
    "splitted = split_and_load(batch, ctx)\n",
    "print('\\n=== splitted into {} ==={}\\n{}'.format(ctx, splitted[0], splitted[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(batch, params, ctx, lr):\n",
    "    # split the data batch and load them on GPUs\n",
    "    data = split_and_load(batch.data[0], ctx)\n",
    "    label = split_and_load(batch.label[0], ctx)\n",
    "    # run forward on each GPU\n",
    "    with gluon.autograd.record():\n",
    "        losses = [loss(lenet(X, W), Y)\n",
    "                  for X, Y, W in zip(data, label, params)]\n",
    "    # run backward on each gpu\n",
    "    for l in losses:\n",
    "        l.backward()\n",
    "    # aggregate gradient over GPUs\n",
    "    for i in range(len(params[0])):\n",
    "        allreduce([params[c][i].grad for c in range(len(ctx))])\n",
    "    # update parameters with SGD on each GPU\n",
    "    for p in params:\n",
    "        SGD(p, lr/batch.data[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_batch(batch, params, ctx):\n",
    "    data = batch.data[0].as_in_context(ctx[0])\n",
    "    pred = nd.argmax(lenet(data, params[0]), axis=1)\n",
    "    return nd.sum(pred == batch.label[0].as_in_context(ctx[0])).asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.test_utils import get_mnist\n",
    "from mxnet.io import NDArrayIter\n",
    "\n",
    "def run(num_gpus, batch_size, lr):\n",
    "    # the list of GPUs will be used\n",
    "    ctx = [gpu(i) for i in range(num_gpus)]\n",
    "    print('Running on {}'.format(ctx))\n",
    "\n",
    "    # data iterator\n",
    "    mnist = get_mnist()\n",
    "    train_data = NDArrayIter(mnist[\"train_data\"], mnist[\"train_label\"], batch_size)\n",
    "    valid_data = NDArrayIter(mnist[\"test_data\"], mnist[\"test_label\"], batch_size)\n",
    "    print('Batch size is {}'.format(batch_size))\n",
    "\n",
    "    # copy parameters to all GPUs\n",
    "    dev_params = [get_params(params, c) for c in ctx]\n",
    "    for epoch in range(5):\n",
    "        # train\n",
    "        start = time()\n",
    "        train_data.reset()\n",
    "        for batch in train_data:\n",
    "            train_batch(batch, dev_params, ctx, lr)\n",
    "        nd.waitall()  # wait all computations are finished to benchmark the time\n",
    "        print('Epoch %d, training time = %.1f sec'%(epoch, time()-start))\n",
    "\n",
    "        # validating\n",
    "        valid_data.reset()\n",
    "        correct, num = 0.0, 0.0\n",
    "        for batch in valid_data:\n",
    "            correct += valid_batch(batch, dev_params, ctx)\n",
    "            num += batch.data[0].shape[0]\n",
    "        print('         validation accuracy = %.4f'%(correct/num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(1, 64, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
