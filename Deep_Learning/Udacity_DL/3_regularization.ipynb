{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data. For the training data, we use a placeholder that will be \n",
    "    # fed at run time with a training minibatch\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, \n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, \n",
    "                                     shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal((image_size * image_size, num_labels)))\n",
    "    biases = tf.Variable(tf.zeros((num_labels)))\n",
    "    \n",
    "    # Training computation\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Prediction for the training, validation, and test data\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.331808\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 14.5%\n",
      "Minibatch loss at step 500: 2.985050\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 1000: 1.984950\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 1500: 1.202626\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2000: 1.034912\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2500: 1.100692\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 3000: 0.694152\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 87.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset: (offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset: (offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {\n",
    "            tf_train_dataset: batch_data,\n",
    "            tf_train_labels: batch_labels,\n",
    "            beta_regul: 1e-3\n",
    "        }\n",
    "        \n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {\n",
    "          tf_train_dataset: batch_data, \n",
    "          tf_train_labels: batch_labels, \n",
    "          beta_regul: regul\n",
    "      }\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEMCAYAAADEXsFmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1234204a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-layer nural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data. For the training data, we use a placeholder that will be \n",
    "    # fed at run time with a training minibatch\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, \n",
    "                                      shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, \n",
    "                                     shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "            beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 598.693970\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 25.6%\n",
      "Minibatch loss at step 500: 198.247620\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 1000: 114.633354\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1500: 68.432053\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 2000: 42.262886\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2500: 25.364660\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 3000: 15.373071\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.2%\n",
      "Test accuracy: 92.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset: (offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset: (offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {\n",
    "            tf_train_dataset: batch_data,\n",
    "            tf_train_labels: batch_labels,\n",
    "            beta_regul: 1e-3\n",
    "        }\n",
    "        \n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e9f9d2922063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_regul\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mregul\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       _, l, predictions = session.run(\n\u001b[0;32m---> 20\u001b[0;31m         [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0maccuracy_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Python/venvs/data-science/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Python/venvs/data-science/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Python/venvs/data-science/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Developer/Python/venvs/data-science/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/Python/venvs/data-science/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 250.924637\n",
      "Minibatch accuracy: 18.0%\n",
      "Validation accuracy: 32.8%\n",
      "Minibatch loss at step 2: 1056.367065\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy: 41.0%\n",
      "Minibatch loss at step 4: 276.047852\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 55.5%\n",
      "Minibatch loss at step 6: 3.408622\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 8: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 10: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 12: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 16: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 18: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 61.8%\n",
      "Test accuracy: 67.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_bacthes = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_bacthes\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 436.009705\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 35.0%\n",
      "Minibatch loss at step 2: 1002.978271\n",
      "Minibatch accuracy: 38.3%\n",
      "Validation accuracy: 51.9%\n",
      "Minibatch loss at step 4: 344.597778\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 59.6%\n",
      "Minibatch loss at step 6: 60.410004\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 66.6%\n",
      "Minibatch loss at step 8: 2.479553\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 65.7%\n",
      "Minibatch loss at step 10: 3.948230\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 12: 6.087380\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 14: 4.628611\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.4%\n",
      "Minibatch loss at step 16: 2.590599\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 65.5%\n",
      "Minibatch loss at step 18: 2.066358\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 20: 1.658137\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 22: 0.335037\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.3%\n",
      "Minibatch loss at step 24: 1.776973\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 28: 2.870566\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 30: 0.388364\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 32: 0.319091\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 34: 0.694114\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 36: 2.444685\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 38: 1.894053\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 40: 4.144346\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 42: 0.266611\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 44: 2.396052\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 50: 0.018386\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 54: 0.359539\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 58: 0.145468\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 64: 3.170363\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 78: 0.748893\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 82: 4.458466\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 86: 0.690469\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 88: 1.625678\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 96: 0.789182\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 100: 0.248707\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.0%\n",
      "Test accuracy: 74.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # variables\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [image_size * image_size, num_hidden_nodes1],\n",
    "            stddev=np.sqrt(2.0 / (image_size * image_size))\n",
    "        )\n",
    "    )\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [num_hidden_nodes1, num_hidden_nodes2],\n",
    "            stddev=np.sqrt(2.0 / (num_hidden_nodes1))\n",
    "        )\n",
    "    )\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    weights3 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [num_hidden_nodes2, num_labels],\n",
    "            stddev=np.sqrt(2.0 / num_hidden_nodes2)\n",
    "        )\n",
    "    )\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # training computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "    logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=tf_train_labels) + \\\n",
    "            beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "    )\n",
    "    \n",
    "    # optimizer\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # output\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.185713\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 31.6%\n",
      "Minibatch loss at step 500: 1.266649\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1000: 0.874565\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 1500: 0.676978\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 2000: 0.795551\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 2500: 0.662780\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 3000: 0.457554\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 3500: 0.507621\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 4000: 0.623531\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 4500: 0.470235\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 5000: 0.431158\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 5500: 0.455475\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 6000: 0.423736\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 6500: 0.435437\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 7000: 0.382983\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 7500: 0.409546\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 8000: 0.480492\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 8500: 0.472388\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 9000: 0.435019\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Test accuracy: 95.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset: (offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset: (offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {\n",
    "            tf_train_dataset: batch_data,\n",
    "            tf_train_labels: batch_labels,\n",
    "        }\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction],\n",
    "            feed_dict=feed_dict\n",
    "        )\n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## added one more layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # variables\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [image_size * image_size, num_hidden_nodes1],\n",
    "            stddev=np.sqrt(2.0 / (image_size * image_size))\n",
    "        )\n",
    "    )\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [num_hidden_nodes1, num_hidden_nodes2],\n",
    "            stddev=np.sqrt(2.0 / (num_hidden_nodes1))\n",
    "        )\n",
    "    )\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    weights3 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [num_hidden_nodes2, num_hidden_nodes3],\n",
    "            stddev=np.sqrt(2.0 / num_hidden_nodes2)\n",
    "        )\n",
    "    )\n",
    "    biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "    weights4 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [num_hidden_nodes3, num_labels],\n",
    "            stddev=np.sqrt(2.0 / num_hidden_nodes3)\n",
    "        )\n",
    "    )\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \n",
    "    # training computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    #drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "    #drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "    lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "    #drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "    logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=tf_train_labels) + \\\n",
    "            beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "    )\n",
    "    \n",
    "    # optimizer\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # output\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.456061\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 30.0%\n",
      "Minibatch loss at step 500: 1.325977\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 1000: 0.956573\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 1500: 0.754438\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 2000: 0.826775\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 2500: 0.704565\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 3000: 0.453666\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 3500: 0.527943\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 4000: 0.647261\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 4500: 0.459133\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 5000: 0.401172\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 5500: 0.429669\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 6000: 0.385861\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 6500: 0.423831\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 7000: 0.333015\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 7500: 0.366880\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 8000: 0.451465\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 8500: 0.395821\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 9000: 0.399604\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 91.1%\n",
      "Test accuracy: 95.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset: (offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset: (offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {\n",
    "            tf_train_dataset: batch_data,\n",
    "            tf_train_labels: batch_labels,\n",
    "        }\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction],\n",
    "            feed_dict=feed_dict\n",
    "        )\n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# added dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # variables\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [image_size * image_size, num_hidden_nodes1],\n",
    "            stddev=np.sqrt(2.0 / (image_size * image_size))\n",
    "        )\n",
    "    )\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [num_hidden_nodes1, num_hidden_nodes2],\n",
    "            stddev=np.sqrt(2.0 / (num_hidden_nodes1))\n",
    "        )\n",
    "    )\n",
    "    biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    weights3 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [num_hidden_nodes2, num_hidden_nodes3],\n",
    "            stddev=np.sqrt(2.0 / num_hidden_nodes2)\n",
    "        )\n",
    "    )\n",
    "    biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "    weights4 = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [num_hidden_nodes3, num_labels],\n",
    "            stddev=np.sqrt(2.0 / num_hidden_nodes3)\n",
    "        )\n",
    "    )\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \n",
    "    # training computation\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "    lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "    drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "    lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "    drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "    logits = tf.matmul(drop3, weights4) + biases4\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=tf_train_labels) + \\\n",
    "            beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "    )\n",
    "    \n",
    "    # optimizer\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # output\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "    lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.852682\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 18.7%\n",
      "Minibatch loss at step 500: 1.603865\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1000: 1.194526\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1500: 0.955791\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 2000: 0.943270\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 2500: 0.896915\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 3000: 0.593469\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 3500: 0.697932\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 4000: 0.731604\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 4500: 0.832615\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 5000: 0.568346\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 5500: 0.576022\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 6000: 0.550590\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 6500: 0.520254\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 7000: 0.463274\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 7500: 0.569346\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 8000: 0.623624\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 8500: 0.666296\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 9000: 0.480694\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.8%\n",
      "Test accuracy: 94.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        batch_data = train_dataset[offset: (offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset: (offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {\n",
    "            tf_train_dataset: batch_data,\n",
    "            tf_train_labels: batch_labels,\n",
    "        }\n",
    "        _, l, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction],\n",
    "            feed_dict=feed_dict\n",
    "        )\n",
    "        if step % 500 == 0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3 Data Science",
   "language": "python",
   "name": "data-scence"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
