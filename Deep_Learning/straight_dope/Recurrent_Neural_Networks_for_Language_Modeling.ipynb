{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "import numpy as np\n",
    "mx.random.seed(1)\n",
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../data/nlp/timemachine.txt\") as f:\n",
    "    time_machine = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(time_machine[0: 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(time_machine[-38075:-37500])\n",
    "time_machine = time_machine[:-38083]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "character_list = list(set(time_machine))\n",
    "vocab_size = len(character_list)\n",
    "print(character_list)\n",
    "print(\"Length of vocab: %s\" % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "character_dict = {}\n",
    "for e, char in enumerate(character_list):\n",
    "    character_dict[char] = e\n",
    "print(character_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_numerical = [character_dict[char] for char in time_machine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(time_numerical))\n",
    "\n",
    "print(time_numerical[:20])\n",
    "\n",
    "print(\"\".join([character_list[idx] for idx in time_numerical[:39]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hots(numerical_list, vocab_size=vocab_size):\n",
    "    result = nd.zeros((len(numerical_list), vocab_size), ctx=ctx)\n",
    "    for i, idx in enumerate(numerical_list):\n",
    "        result[i, idx] = 1.0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(one_hots(time_numerical[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textify(embedding):\n",
    "    result = \"\"\n",
    "    indices = nd.argmax(embedding, axis=1).asnumpy()\n",
    "    for idx in indices:\n",
    "        result += character_list[int(idx)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textify(one_hots(time_numerical[0: 40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_length = 64\n",
    "num_samples = (len(time_numerical) - 1) // seq_length\n",
    "dataset = one_hots(time_numerical[: seq_length * num_samples]).reshape((num_samples, seq_length, vocab_size))\n",
    "textify(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"# of sequences in dataset: \", len(dataset))\n",
    "num_batches = len(dataset) // batch_size\n",
    "print(\"# of batches: \", num_batches)\n",
    "train_data = dataset[: num_batches * batch_size].reshape((batch_size, num_batches, seq_length, vocab_size))\n",
    "\n",
    "train_data = nd.swapaxes(train_data, 0, 1)\n",
    "train_data = nd.swapaxes(train_data, 1, 2)\n",
    "print(\"Shape of data set: \", train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(\"***Batch %s:***\\n %s \\n %s \\n\\n\" % (i, textify(train_data[i, :, 0]), textify(train_data[i, :, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = one_hots(time_numerical[1: seq_length * num_samples + 1])\n",
    "train_label = labels.reshape((batch_size, num_batches, seq_length, vocab_size))\n",
    "train_label = nd.swapaxes(train_label, 0, 1)\n",
    "train_label = nd.swapaxes(train_label, 1, 2)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(textify(train_data[10, :, 3]))\n",
    "print(textify(train_label[10, :, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs = vocab_size\n",
    "num_hidden = 256\n",
    "num_outputs = vocab_size\n",
    "\n",
    "Wxh = nd.random_normal(shape=(num_inputs, num_hidden), ctx=ctx) * 0.01\n",
    "\n",
    "Whh = nd.random_normal(shape=(num_hidden, num_hidden), ctx=ctx) * 0.01\n",
    "\n",
    "bn = nd.random_normal(shape=num_hidden, ctx=ctx) * 0.01\n",
    "\n",
    "Why = nd.random_normal(shape=(num_hidden, num_outputs), ctx=ctx) * 0.01\n",
    "\n",
    "by = nd.random_normal(shape=num_outputs, ctx=ctx) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = [Wxh, Whh, bh, Why, by]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(y_linear, temperature=1.0):\n",
    "    lin = (y_linear - nd.max(y_linear, axis=1).reshape((-1, 1))) / temperature\n",
    "    exp = nd.exp(lin)\n",
    "    partition = nd.sum(exp, axis=1).reshape((-1, 1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax(nd.array([[1, -1], [-1, 1]]), temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax(nd.array([[1,-1],[-1,1]]), temperature=1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax(nd.array([[10,-10],[-10,10]]), temperature=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_rnn(inputs, state, temperature=1.0):\n",
    "    outputs = []\n",
    "    h = state\n",
    "    for X in inputs:\n",
    "        h_linear = nd.dot(X, Wxh) + nd.dot(h, Whh) + bn\n",
    "        h = nd.tanh(h_linear)\n",
    "        yhat_linear = nd.dot(h, Why) + by\n",
    "        yhat = softmax(yhat_linear, temperature=temperature)\n",
    "        outputs.append(yhat)\n",
    "    return (outputs, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    return - nd.mean(nd.sum(y * nd.log(yhat), axis=0, exclude=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy(nd.array([[.2,.5,.3], [.2,.5,.3]]), nd.array([[1.,0,0], [0, 1.,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_ce_loss(outputs, labels):\n",
    "    assert(len(outputs) == len(labels))\n",
    "    total_loss = 0\n",
    "    for (output, label) in zip(outputs, labels):\n",
    "        total_loss = total_loss + cross_entropy(output, label)\n",
    "    return total_loss / len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(prefix, num_chars, temperature=1.0):\n",
    "    string = prefix\n",
    "    \n",
    "    prefix_numerical = [character_dict[char] for char in prefix]\n",
    "    input = ont_hots(prefix_numerical)\n",
    "    \n",
    "    sample_state = nd.zeros(shape=(1, num_hidden), ctx=ctx)\n",
    "    \n",
    "    for i in range(num_chars):\n",
    "        outputs, sample_state = simple_rnn(input, sample_state, temperature=temperature)\n",
    "        choice = np.random.choice(vocab_size, p=outputs[-1][0].asnumpy())\n",
    "        string += character_list[choice]\n",
    "        input = one_hots([choice])\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 2000\n",
    "moving_loss = 0.\n",
    "\n",
    "learning_rate = .5\n",
    "\n",
    "# state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
    "for e in range(epochs):\n",
    "    ############################\n",
    "    # Attenuate the learning rate by a factor of 2 every 100 epochs.\n",
    "    ############################\n",
    "    if ((e+1) % 100 == 0):\n",
    "        learning_rate = learning_rate / 2.0\n",
    "    state = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)\n",
    "    for i in range(num_batches):\n",
    "        data_one_hot = train_data[i]\n",
    "        label_one_hot = train_label[i]\n",
    "        with autograd.record():\n",
    "            outputs, state = simple_rnn(data_one_hot, state)\n",
    "            loss = average_ce_loss(outputs, label_one_hot)\n",
    "            loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        if (i == 0) and (e == 0):\n",
    "            moving_loss = np.mean(loss.asnumpy()[0])\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * np.mean(loss.asnumpy()[0])\n",
    "\n",
    "    print(\"Epoch %s. Loss: %s\" % (e, moving_loss))\n",
    "    print(sample(\"The Time Ma\", 1024, temperature=.1))\n",
    "    print(sample(\"The Medical Man rose, came to the lamp,\", 1024, temperature=.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
