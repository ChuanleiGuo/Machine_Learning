{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo = True\n",
    "log_interval = 100\n",
    "gpus = 0\n",
    "\n",
    "mode = \"hybrid\"\n",
    "\n",
    "batch_size = 256\n",
    "if demo:\n",
    "    epochs = 5\n",
    "    learning_rate = 0.02\n",
    "    wd = 0.002\n",
    "else:\n",
    "    epochs = 40\n",
    "    learning_rate = 0.05\n",
    "    wd = 0.002\n",
    "    \n",
    "positive_class_weight = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import os\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import skimage.io as io\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet.test_utils import download\n",
    "mx.random.seed(127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_files = {'train': ('not_hotdog_train-e6ef27b4.rec', '0aad7e1f16f5fb109b719a414a867bbee6ef27b4'),\n",
    "                 'validation': ('not_hotdog_validation-c0201740.rec', '723ae5f8a433ed2e2bf729baec6b878ac0201740')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if demo:\n",
    "    training_dataset, training_data_hash = dataset_files[\"validation\"]\n",
    "else:\n",
    "    training_dataset, training_data_hash = dataset_files[\"train\"]\n",
    "    \n",
    "validation_dataset, validation_data_hash = dataset_files[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verified(file_path, sha1hash):\n",
    "    import hashlib\n",
    "    sha1 = hashlib.sha1()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        while True:\n",
    "            data = f.read(1048576)\n",
    "            if not data:\n",
    "                break\n",
    "            sha1.update(data)\n",
    "    matched = sha1.hexdigest() == sha1hash\n",
    "    if not matched:\n",
    "        logging.warn('Found hash mismatch in file {}, possibly due to incomplete download.'\n",
    "                     .format(file_path))\n",
    "    return matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_format = 'https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/{}'\n",
    "if not os.path.exists(training_dataset) or not verified(training_dataset, training_data_hash):\n",
    "    logging.info('Downloading training dataset.')\n",
    "    download(url_format.format(training_dataset),\n",
    "             overwrite=True)\n",
    "if not os.path.exists(validation_dataset) or not verified(validation_dataset, validation_data_hash):\n",
    "    logging.info('Downloading validation dataset.')\n",
    "    download(url_format.format(validation_dataset),\n",
    "             overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter = mx.io.ImageRecordIter(\n",
    "    path_imgrec=training_dataset,\n",
    "    min_img_size=256,\n",
    "    data_shape=(3, 224, 224),\n",
    "    rand_crop=True,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    max_random_scale=1.5,\n",
    "    min_random_scale=0.75,\n",
    "    rand_mirror=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_iter = mx.io.ImageRecordIter(\n",
    "    path_imgrec=validation_dataset,\n",
    "    min_img_size=256,\n",
    "    data_shape=(3, 224, 224),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.model_zoo import vision as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.squeezenet1_1(pretrained=True, prefix=\"deep_dog_\")\n",
    "imagenet_hotdog_index = 713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_dog_net = models.squeezenet1_1(prefix='deep_dog_', classes=2)\n",
    "deep_dog_net.collect_params().initialize()\n",
    "deep_dog_net.features = net.features\n",
    "print(deep_dog_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.color import rgba2rgb\n",
    "\n",
    "def classify_hotdog(net, url):\n",
    "    I = io.imread(url)\n",
    "    if I.shape[2] == 4:\n",
    "        I = rgba2rgb(I)\n",
    "    image = mx.nd.array(I).astype(np.uint8)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.show(image.asnumpy())\n",
    "    image = mx.image.resize_short(image, 256)\n",
    "    image, _ = mx.image.center_crop(image, (224, 224))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image.asnumpy())\n",
    "    image = mx.image.color_normalize(\n",
    "        image.astype(np.float32) / 255,\n",
    "        mean=mx.nd.array([0.485, 0.456, 0.406]),\n",
    "        std=md.nx.array([0.229, 0.224, 0.225])\n",
    "    )\n",
    "    image = mx.nd.transpose(image.astype(\"float32\"), (2, 1, 0))\n",
    "    image = mx.nd.expand_dims(image, axis=0)\n",
    "    out = mx.nd.SoftmaxActivation(net(image))\n",
    "    print('Probabilities are: '+str(out[0].asnumpy()))\n",
    "    result = np.argmax(out.asnumpy())\n",
    "    outstring = ['Not hotdog!', 'Hotdog!']\n",
    "    print(outstring[result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"./img/real_hotdog.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = mx.nd.array(img).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mx.image.resize_short(image, 256)\n",
    "image, _ = mx.image.center_crop(image, (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mx.image.color_normalize(\n",
    "        image.astype(np.float32) / 255,\n",
    "        mean=mx.nd.array([0.485, 0.456, 0.406]),\n",
    "        std=mx.nd.array([0.229, 0.224, 0.225])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mx.nd.transpose(image.astype(\"float32\"), (2, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mx.nd.expand_dims(image, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_conv_params = net.classifier[1].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_conv_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_weight_param = pretrained_conv_params.get('weight')\n",
    "pretrained_bias_param = pretrained_conv_params.get('bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hotdog_w = mx.nd.split(pretrained_weight_param.data().as_in_context(mx.cpu()),\n",
    "                       1000, axis=0)[imagenet_hotdog_index]\n",
    "hotdog_b = mx.nd.split(pretrained_bias_param.data().as_in_context(mx.cpu()),\n",
    "                       1000, axis=0)[imagenet_hotdog_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_classifier_w = mx.nd.concat(mx.nd.random_normal(shape=hotdog_w.shape, scale=0.02),\n",
    "                                hotdog_w,\n",
    "                                dim=0)\n",
    "new_classifier_b = mx.nd.concat(mx.nd.random_normal(shape=hotdog_b.shape, scale=0.02),\n",
    "                                hotdog_b,\n",
    "                                dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_conv_layer_params = deep_dog_net.classifier[1].params\n",
    "final_conv_layer_params.get('weight').set_data(new_classifier_w)\n",
    "final_conv_layer_params.get('bias').set_data(new_classifier_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metric_str(names, accs):\n",
    "    return ', '.join(['%s=%f'%(name, acc) for name, acc in zip(names, accs)])\n",
    "metric = mx.metric.create(['acc', 'f1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet.gluon as gluon\n",
    "from mxnet.image import color_normalize\n",
    "\n",
    "def evaluate(net, data_iter, ctx):\n",
    "    data_iter.reset()\n",
    "    for batch in data_iter:\n",
    "        data = color_normalize(batch.data[0]/255,\n",
    "                               mean=mx.nd.array([0.485, 0.456, 0.406]).reshape((1,3,1,1)),\n",
    "                               std=mx.nd.array([0.229, 0.224, 0.225]).reshape((1,3,1,1)))\n",
    "        data = gluon.utils.split_and_load(data, ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = []\n",
    "        for x in data:\n",
    "            outputs.append(net(x))\n",
    "        metric.update(label, outputs)\n",
    "    out = metric.get()\n",
    "    metric.reset()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet.autograd as autograd\n",
    "\n",
    "def train(net, train_iter, val_iter, epochs, ctx):\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': learning_rate, 'wd': wd})\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    best_f1 = 0\n",
    "    val_names, val_accs = evaluate(net, val_iter, ctx)\n",
    "    logging.info('[Initial] validation: %s'%(metric_str(val_names, val_accs)))\n",
    "    for epoch in range(epochs):\n",
    "        tic = time.time()\n",
    "        train_iter.reset()\n",
    "        btic = time.time()\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            # the model zoo models expect normalized images\n",
    "            data = color_normalize(batch.data[0]/255,\n",
    "                                   mean=mx.nd.array([0.485, 0.456, 0.406]).reshape((1,3,1,1)),\n",
    "                                   std=mx.nd.array([0.229, 0.224, 0.225]).reshape((1,3,1,1)))\n",
    "            data = gluon.utils.split_and_load(data, ctx_list=ctx, batch_axis=0)\n",
    "            label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "            outputs = []\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                for x, y in zip(data, label):\n",
    "                    z = net(x)\n",
    "                    # rescale the loss based on class to counter the imbalance problem\n",
    "                    L = loss(z, y) * (1+y*positive_class_weight)/positive_class_weight\n",
    "                    # store the loss and do backward after we have done forward\n",
    "                    # on all GPUs for better speed on multiple GPUs.\n",
    "                    Ls.append(L)\n",
    "                    outputs.append(z)\n",
    "                for L in Ls:\n",
    "                    L.backward()\n",
    "            trainer.step(batch.data[0].shape[0])\n",
    "            metric.update(label, outputs)\n",
    "            if log_interval and not (i+1)%log_interval:\n",
    "                names, accs = metric.get()\n",
    "                logging.info('[Epoch %d Batch %d] speed: %f samples/s, training: %s'%(\n",
    "                               epoch, i, batch_size/(time.time()-btic), metric_str(names, accs)))\n",
    "            btic = time.time()\n",
    "\n",
    "        names, accs = metric.get()\n",
    "        metric.reset()\n",
    "        logging.info('[Epoch %d] training: %s'%(epoch, metric_str(names, accs)))\n",
    "        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n",
    "        val_names, val_accs = evaluate(net, val_iter, ctx)\n",
    "        logging.info('[Epoch %d] validation: %s'%(epoch, metric_str(val_names, val_accs)))\n",
    "\n",
    "        if val_accs[1] > best_f1:\n",
    "            best_f1 = val_accs[1]\n",
    "            logging.info('Best validation f1 found. Checkpointing...')\n",
    "            net.save_params('deep-dog-%d.params'%(epoch))\n",
    "\n",
    "if mode == 'hybrid':\n",
    "    deep_dog_net.hybridize()\n",
    "if epochs > 0:\n",
    "    contexts = [mx.gpu(i) for i in range(gpus)] if gpus > 0 else [mx.cpu()]\n",
    "    deep_dog_net.collect_params().reset_ctx(contexts)\n",
    "    train(deep_dog_net, train_iter, val_iter, epochs, contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
